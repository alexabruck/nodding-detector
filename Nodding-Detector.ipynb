{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd7b220-bdcd-4549-84ce-f00fed5a21c1",
   "metadata": {},
   "source": [
    "# Nodding Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096917a8-c811-4f60-a977-a513c89c95c1",
   "metadata": {},
   "source": [
    "This notebook builds upon the notebook \"Action detection for sign language\" by Nicholas Renotte and adapts it to the usecase of detecting nodding gestures:\n",
    "https://github.com/nicknochnack/ActionDetectionforSignLanguage/blob/main/Action%20Detection%20Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d3a8a-d92b-4d1a-9d9c-7bab22d27fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66668a-dea6-4b9c-bb0d-d61103b45fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe tensorflow==2.4.1 opencv-python sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9460a14-40b2-4a9c-ab95-d9f277407f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn #sklearn vs. scikit-learn, see here: https://towardsdatascience.com/scikit-learn-vs-sklearn-6944b9dc1736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0ba26-9c8b-4a50-a1a7-78a8c6e2d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761680f1-0f40-4e5c-b933-f9d043e1495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_facemesh = mp.solutions.face_mesh # Only Facemesh instead of Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702ae68-89c0-45ab-8e50-bd5ad345f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cafa46-7cba-4760-bab1-4a2bd504af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    if not results:\n",
    "        return\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.multi_face_landmarks[0], mp_facemesh.FACEMESH_TESSELATION) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b801a-a606-4a41-b24c-ced7273c0554",
   "metadata": {},
   "source": [
    "## 1. Testing the capturing of facial landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6137389-27e2-4093-b5bb-d6d2f479c813",
   "metadata": {},
   "source": [
    "Note: On Mac, the cv2.destroyAllWindows() doesn't work. The window stays open and freezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd6a96-3693-4af6-a4c7-58cf5d13cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_facemesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, static_image_mode=True, max_num_faces=1, refine_landmarks=False,) as face:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, face)\n",
    "        if results and results.multi_face_landmarks:    \n",
    "            print(results.multi_face_landmarks[0].landmark[0].x)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    #cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5435667-0d0f-426d-a2f8-3ef9fd23d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_frame_keypoints = results.multi_face_landmarks[0].landmark\n",
    "len(last_frame_keypoints) # number of keypoints detected in one frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292988e-80cf-435a-a162-a75d57cb64eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Extract keypoint values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5cc8dc-00dc-458b-8f12-47ae13eb3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_pt = singleresult[1]\n",
    "aligned_landmarks = [{\"x\": pt.x - ref_pt.x, \"y\": pt.y - ref_pt.y, \"z\": pt.z - ref_pt.z} for pt in singleresult]\n",
    "aligned_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe050da-cd17-4084-b1ce-14ec3edc5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def normalize_landmarks(landmarks, reference_size=256):\n",
    "    #align_and_scale\n",
    "    ref_pt = landmarks[1] #Nosetip\n",
    "    aligned_landmarks = [{\"x\": pt.x - ref_pt.x, \"y\": pt.y - ref_pt.y, \"z\": pt.z - ref_pt.z} for pt in landmarks]\n",
    "    scale_factor = reference_size / np.max(np.abs(aligned_landmarks))\n",
    "    scaled_landmarks = aligned_landmarks * scale_factor\n",
    "    return scaled_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d43e7f-2754-40e3-8401-fb813cd4aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    #TODO: normalize first\n",
    "    NOSE_INDEX = 1\n",
    "    relevant_points = [NOSE_INDEX, 2, 3] #TODO: Select more expressive points\n",
    "    data = [0, 0, 0] * len(relevant_points) #fallback if no data\n",
    "    if results.multi_face_landmarks[0].landmark:\n",
    "        data = np.array([[res.x, res.y, res.z] for res in np.array(results.multi_face_landmarks[0].landmark)[[relevant_points]]]).flatten()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547d61d-1a0a-43b2-a196-9349e32186d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Setup folders to store training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e5522-eded-4f00-a841-07a235f955f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "ACTIONS = np.array(['nodding', 'naying', 'random'])\n",
    "\n",
    "# Thirty videos per action\n",
    "N_SEQUENCES = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3d822-98df-475b-997c-4a0495dd9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in ACTIONS: \n",
    "    for sequence in range(N_SEQUENCES):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838c9d1-baa5-4336-997c-1911cbfd13cf",
   "metadata": {},
   "source": [
    "## 4. Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377e8ec-52c5-4be2-af12-97903850b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_facemesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, static_image_mode=True, max_num_faces=1, refine_landmarks=False,) as face:\n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in ACTIONS:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(N_SEQUENCES):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(SEQUENCE_LENGTH):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, face)\n",
    "#                 print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'PREPARE FOR RECORDING', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'RECORDING NOW', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ddf38e-9323-4078-8ab1-5ed792a775f9",
   "metadata": {},
   "source": [
    "## 5. Visualize data to check if there is anything odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df890f80-324a-4646-a489-220eb2f3cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "for action in ACTIONS:\n",
    "    plt.title(f'{action} graph')\n",
    "    for sequence in range(N_SEQUENCES):\n",
    "        window = []\n",
    "        for frame_num in range(SEQUENCE_LENGTH):\n",
    "            try:\n",
    "                res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "                window.append(res)\n",
    "            except:\n",
    "                pass\n",
    "        #visualize window\n",
    "        nose_x = [r[0] for r in window]\n",
    "        nose_y = [r[1] for r in window]\n",
    "        nose_z = [r[2] for r in window]\n",
    "        colorX = \"green\" if action == \"naying\" else \"red\"\n",
    "        colorY = \"green\" if action == \"nodding\" else \"red\"\n",
    "        plt.plot(nose_x, color=colorX)\n",
    "        plt.plot(nose_y, color=colorY)\n",
    "        #plt.plot(nose_z, color=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcce2c-5922-41cf-8ae4-11dcfa19fbe4",
   "metadata": {},
   "source": [
    "## 6. Preprocess Data and Create Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340b3c9-4131-41f2-9c23-4444a2f551b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8b391-88e4-4087-91b3-fd460c8908b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(ACTIONS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d198da-175d-4966-b7af-1f0fe53217ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in ACTIONS:\n",
    "    for sequence in range(N_SEQUENCES):\n",
    "        if not os.path.isdir(os.path.join(DATA_PATH, action, str(sequence))): #in case we manually deleted some recordings\n",
    "            continue\n",
    "        window = []\n",
    "        for frame_num in range(SEQUENCE_LENGTH):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a53676-3248-4d8d-b186-4db4a64559ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd904c-c451-4fe0-b295-bff2deaaa080",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c26820-33e5-40a5-acab-673cc16b397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466adbc2-4b41-4dbd-a34f-1c56bea3129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff29d1-d117-485d-a5b0-e433178a6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ab1eb-d140-46d3-8a07-0909f3d8e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce27219-d54e-449e-8792-5e3ea16ed776",
   "metadata": {},
   "source": [
    "## 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ee526-4211-4fb5-a665-db4f81fb9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476c6a5-7a87-4672-8ba4-98a674677078",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5eaf33-48df-4088-bcbd-4ebdaf785cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu', input_shape=(30,9)))\n",
    "#model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "#model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(ACTIONS.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e9ee7-3fea-47d9-99e6-0f3805e4e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ba4f9-2bc4-4a6b-85a6-5ebe342c21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wichtig! Use binary_crossentropy statt categorical_crossentropy wenn ich nur 2 Labels hab (nodding & nicht -nodding)!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4974788-3571-4677-b498-71f53daf2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdab6df-2309-4a18-9260-f00d0e2f679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054b054-b718-422a-a857-d498742288c9",
   "metadata": {},
   "source": [
    "## 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca48f7-b2a3-4681-8ab2-9d40c1af123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797b353-cd6e-4359-b88d-f716b69969d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4db25-0712-41fe-94c2-7e96774bdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS[np.argmax(res[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee7f96-968b-4359-8ef6-e5c4cb34c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS[np.argmax(y_test[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914d3ee-7828-4e6c-b222-a0dac14ab56b",
   "metadata": {},
   "source": [
    "## 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d172d-3772-4712-8f2d-5f15642c783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3dbff7-4e2d-4a9a-9e70-da30b26c1517",
   "metadata": {},
   "source": [
    "## 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec9a9a-8fbc-4276-a804-bdad8c21758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffecd250-17c0-4edd-a012-ece25ea18a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48434c5-0d2a-4bf2-be0c-03b1fa065616",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd74a2-8f5f-4a4a-993a-9a7d0c79d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874f7b0-3e17-41e5-b4e6-51f2b080ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b988afa-921f-49ef-ae6b-2c9ddad472d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ecc30-e525-4eb3-af7f-6fca6733a032",
   "metadata": {},
   "source": [
    "## (Optional) Load weights into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d68f2-b1b5-481a-8daa-afa4fa8ece77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da90152-c1c7-45d6-87dd-fe397bf77b93",
   "metadata": {},
   "source": [
    "## 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc15140-534a-486e-af9b-4dad76efabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299063bc-d513-4d48-acab-084848a4ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0da166-4756-4975-8237-313fbaaba827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_facemesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, static_image_mode=True, max_num_faces=1, refine_landmarks=False,) as face:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, face)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(ACTIONS[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if ACTIONS[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(ACTIONS[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(ACTIONS[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, ACTIONS, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8911e8-840f-4339-b8a1-7140fd714e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
